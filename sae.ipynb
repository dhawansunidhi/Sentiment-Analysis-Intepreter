{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataPreprocessing import *\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x118d37710>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, model_name, loss_fn='ce'):\n",
    "    file_path = os.path.join(os.getcwd(), 'model_weights', f'checkpoint_{model_name}_{loss_fn}.pt')\n",
    "    os.makedirs(os.path.join(os.getcwd(), 'model_weights'), exist_ok=True)\n",
    "    checkpoint = { # create a dictionary with all the state information\n",
    "        'model_state_dict': model.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)\n",
    "    print(f\"Checkpoint saved to {file_path}\")\n",
    "\n",
    "def load_checkpoint(model, model_name, loss_fn='ce', map_location='cpu'):\n",
    "    file_path = os.path.join(os.getcwd(), 'model_weights', f'checkpoint_{model_name}_{loss_fn}.pt')\n",
    "    checkpoint = torch.load(file_path, map_location=map_location) # load the checkpoint, ensure correct device\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss_over_time, val_loss_over_time, model_name):\n",
    "    epochs = range(1, len(train_loss_over_time) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_loss_over_time, color='red', label='Train Loss')\n",
    "    plt.plot(epochs, val_loss_over_time, color='blue', label='Val Loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Loss for {model_name}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder path were dataset is located\n",
    "path = 'dataset/'\n",
    "#initialize empty lists to hold data\n",
    "train_pos, train_neg, test_pos, test_neg = [], [], [], []\n",
    "#create a dictionary where the key is the relative path to data and value is empty list\n",
    "sets_dict = {'train/pos/': train_pos, 'train/neg/': train_neg, 'test/pos/': test_pos, 'test/neg/': test_neg}\n",
    "#loop through dictionary to read from files and populate empty lists\n",
    "for dataset in sets_dict:\n",
    "        file_list = [file for file in os.listdir(os.path.join(path, dataset)) if file.endswith('.txt')]\n",
    "        file_list = sorted(file_list)\n",
    "        load_data(os.path.join(path, dataset), file_list, sets_dict[dataset])\n",
    "#Covert lists to pandas dataframes and combine to form train and test datasets\n",
    "train_data = pd.concat([pd.DataFrame({'review': train_pos, 'label':1}), pd.DataFrame({'review': train_neg, 'label':0})], axis = 0, ignore_index=True)\n",
    "test_data = pd.concat([pd.DataFrame({'review': test_pos, 'label':1}), pd.DataFrame({'review': test_neg, 'label':0})], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "                                              review  label\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...      1\n",
      "1  Homelessness (or Houselessness as George Carli...      1\n",
      "2  Brilliant over-acting by Lesley Ann Warren. Be...      1\n",
      "3  This is easily the most underrated film inn th...      1\n",
      "4  This is not the typical Mel Brooks film. It wa...      1\n",
      "                                                  review  label\n",
      "24995  Towards the end of the movie, I felt it was to...      0\n",
      "24996  This is the kind of movie that my enemies cont...      0\n",
      "24997  I saw 'Descent' last night at the Stockholm Fi...      0\n",
      "24998  Some films that you pick up for a pound turn o...      0\n",
      "24999  This is one of the dumbest films, I've ever se...      0\n"
     ]
    }
   ],
   "source": [
    "#Visualize train_data dataframe\n",
    "print(train_data.shape)\n",
    "print(train_data.head())\n",
    "print(train_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "                                              review  label\n",
      "0  I went and saw this movie last night after bei...      1\n",
      "1  Actor turned director Bill Paxton follows up h...      1\n",
      "2  As a recreational golfer with some knowledge o...      1\n",
      "3  I saw this film in a sneak preview, and it is ...      1\n",
      "4  Bill Paxton has taken the true story of the 19...      1\n",
      "                                                  review  label\n",
      "24995  I occasionally let my kids watch this garbage ...      0\n",
      "24996  When all we have anymore is pretty much realit...      0\n",
      "24997  The basic genre is a thriller intercut with an...      0\n",
      "24998  Four things intrigued me as to this film - fir...      0\n",
      "24999  David Bryce's comments nearby are exceptionall...      0\n"
     ]
    }
   ],
   "source": [
    "#Visualize test_data dataframe\n",
    "print(test_data.shape)\n",
    "print(test_data.head())\n",
    "print(test_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"tokenized\"] = train_data[\"review\"].apply(lambda x: tokenize(clean_text(x.lower())))\n",
    "test_data[\"tokenized\"] = test_data[\"review\"].apply(lambda x: tokenize(clean_text(x.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [bromwell, high, is, a, cartoon, comedy, ., it...\n",
      "1    [homelessness, (, or, houselessness, as, georg...\n",
      "2    [brilliant, over, -, acting, by, lesley, ann, ...\n",
      "3    [this, is, easily, the, most, underrated, film...\n",
      "4    [this, is, not, the, typical, mel, brooks, fil...\n",
      "Name: tokenized, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Examine tokenized reviews\n",
    "print(train_data.head()[\"tokenized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2772\n",
      "273.77724\n",
      "4537\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "total = 0\n",
    "above_thresh = 0\n",
    "for review in train_data[\"tokenized\"]:\n",
    "  if len(review) > max:\n",
    "    max = len(review)\n",
    "  total += len(review)\n",
    "  if len(review) > 400:\n",
    "    above_thresh += 1\n",
    "print(max)\n",
    "print(total/25000)\n",
    "print(above_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab, reversed_train_vocab = generate_vocab_map(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator().manual_seed(RANDOM_SEED)\n",
    "\n",
    "train_dataset = ReviewDataset(train_vocab, train_data)\n",
    "train_dataset, val_dataset = random_split(train_dataset,[0.9,0.1], generator=generator1)\n",
    "test_dataset  = ReviewDataset(train_vocab, test_data)\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler = RandomSampler(val_dataset)\n",
    "test_sampler  = RandomSampler(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
    "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSentimentTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(SimpleSentimentTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = nn.Embedding(400, embed_dim) # max sequence length = 400\n",
    "        encoder_layers = TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=embed_dim, dropout=dropout)\n",
    "        # self.encoder_norm = nn.LayerNorm(embed_dim)\n",
    "        # self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers, norm=self.encoder_norm)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, 2) # num_classes = 2\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_length = x.size(1)\n",
    "        pos = torch.arange(0, seq_length).unsqueeze(0).repeat(x.size(0), 1).to(x.device)\n",
    "        x = self.embedding(x) + self.pos_encoder(pos)\n",
    "        x = x.permute(1, 0, 2)  # change to (seq_len, batch_size, embed_dim)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)  # average pooling over sequence length\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_and_f1_score(y_true, y_predicted):\n",
    "    \"\"\"\n",
    "    This function takes in two numpy arrays and computes the accuracy and F1 score\n",
    "    between them. You can use the imported sklearn functions to do this.\n",
    "\n",
    "    Args:\n",
    "        y_true (list) : A 1D numpy array of ground truth labels\n",
    "        y_predicted (list) : A 1D numpy array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float) : The accuracy of the predictions\n",
    "        f1_score (float) : The F1 score of the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_predicted)\n",
    "\n",
    "    f1 = f1_score(y_true, y_predicted, average='macro')\n",
    "\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(classes)))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity_penalty_kl(encoded):\n",
    "        rho_hat = torch.mean(encoded, dim=0)\n",
    "        rho = self.sparsity_target\n",
    "        epsilon = 1e-8\n",
    "        rho_hat = torch.clamp(rho_hat, min=epsilon, max=1 - epsilon)\n",
    "        kl_divergence = rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat))\n",
    "        sparsity_penalty = torch.sum(kl_divergence)\n",
    "        return self.sparsity_lambda * sparsity_penalty\n",
    "\n",
    "def sparsity_penalty_l1(encoded):\n",
    "        return torch.sum(encoded, dim=1).mean()\n",
    "\n",
    "def sparse_reconstruction_loss(x_hat, x, encoded):\n",
    "        sparsity_penalty = sparsity_penalty_l1(encoded)\n",
    "        mse_loss = F.mse_loss(x_hat, x, reduction='mean')\n",
    "        return sparsity_penalty + mse_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_criterion(loss_type='ce'):\n",
    "    criterion = None\n",
    "\n",
    "    ## YOUR CODE STARTS HERE ##\n",
    "    if loss_type == 'ce':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif loss_type == 'sr':\n",
    "        criterion = sparse_reconstruction_loss\n",
    "    elif loss_type == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, learning_rate):\n",
    "    \"\"\"\n",
    "    This function takes a model and a learning rate, and returns an optimizer.\n",
    "    Feel free to experiment with different optimizers.\n",
    "    \"\"\"\n",
    "    optimizer = None\n",
    "\n",
    "    ## YOUR CODE STARTS HERE ##\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, criterion, optimizer, iterator, epoch, save_every=10):\n",
    "    \"\"\"\n",
    "    This function is used to train a model for one epoch.\n",
    "    :param model: The model to be trained\n",
    "    :param criterion: The loss function\n",
    "    :param optim: The optimizer\n",
    "    :param iterator: The training data iterator\n",
    "    :return: The average loss for this epoch\n",
    "    \"\"\"\n",
    "    model.train() # Is used to put the model in training mode\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(iterator, total=len(iterator), desc=\"Training Model\"):\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        # remove this when you add your implementation\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "\n",
    "        # output = output.long()\n",
    "        y = y.long()\n",
    "\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(model, criterion, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate a model on the validation set.\n",
    "    :param model: The model to be evaluated\n",
    "    :param iterator: The validation data iterator\n",
    "    :return: true: a Python boolean array of all the ground truth values\n",
    "             pred: a Python boolean array of all model predictions.\n",
    "            average_loss: The average loss over the validation set\n",
    "    \"\"\"\n",
    "\n",
    "    true, pred = [], []\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    for x, y in tqdm(iterator, total=len(iterator), desc=\"Evaluating Model\"):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "         # remove this when you add your implementation\n",
    "         x, y = x.to(device), y.to(device)\n",
    "         output = model(x)\n",
    "\n",
    "        #  output = output.long()\n",
    "         y = y.long()\n",
    "\n",
    "         loss = criterion(output, y)\n",
    "\n",
    "         total_loss += loss.item()\n",
    "         true.extend(y.tolist())\n",
    "         predicted = torch.argmax(output, dim=1)\n",
    "         pred.extend(predicted.tolist())\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return true, pred, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model, criterion, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate a model on the test set.\n",
    "    :param model: The model to be evaluated\n",
    "    :param iterator: The validation data iterator\n",
    "    :return: true: a Python boolean array of all the ground truth values\n",
    "             pred: a Python boolean array of all model predictions.\n",
    "            average_loss: The average loss over the validation set\n",
    "    \"\"\"\n",
    "\n",
    "    true, pred = [], []\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    for x, y in tqdm(iterator, total=len(iterator), desc=\"Evaluating Model\"):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "         # remove this when you add your implementation\n",
    "         x, y = x.to(device), y.to(device)\n",
    "         output = model(x)\n",
    "\n",
    "        #  output = output.long()\n",
    "         y = y.long()\n",
    "\n",
    "         loss = criterion(output, y)\n",
    "\n",
    "         total_loss += loss.item()\n",
    "         true.extend(y.tolist())\n",
    "         predicted = torch.argmax(output, dim=1)\n",
    "         pred.extend(predicted.tolist())\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return true, pred, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparams_transformer():\n",
    "    VOCAB_SIZE = len(train_vocab)\n",
    "    EMBED_DIM = 4\n",
    "    NUM_HEADS = 1\n",
    "    NUM_LAYERS = 1\n",
    "    DROPOUT = 0.1\n",
    "    LEARNING_RATE = 0.015\n",
    "    EPOCHS = 2\n",
    "    return VOCAB_SIZE, EMBED_DIM, NUM_HEADS, NUM_LAYERS, DROPOUT, LEARNING_RATE, EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_model(vocab_size, embedding_dim, num_heads, num_layers, dropout):\n",
    "    model = SimpleSentimentTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embedding_dim,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguel/miniconda3/envs/nlp_proj/lib/python3.12/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "Evaluating Model: 100%|██████████| 391/391 [00:17<00:00, 22.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.87276\n",
      "Final Test F1-Score: 0.8726350623323684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE, EMBED_DIM, NUM_HEADS, NUM_LAYERS, DROPOUT, LEARNING_RATE, EPOCHS = get_hyperparams_transformer()\n",
    "\n",
    "transformer_model = get_transformer_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "load_checkpoint(transformer_model, 'transformer2', map_location=device)\n",
    "criterion = get_criterion()\n",
    "\n",
    "# evaluate model\n",
    "true, pred, val_loss = test_loop(transformer_model, criterion, test_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Test Accuracy: {accuracy}\")\n",
    "print(f\"Final Test F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(true, pred, classes=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {'fc': torch.empty((0, 4), device=device)}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = torch.cat([activation[name], torch.flatten(output.detach(), start_dim=0, end_dim=1)]) \n",
    "    return hook\n",
    "\n",
    "handle = transformer_model.transformer_encoder.register_forward_hook(get_activation('fc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model: 100%|██████████| 352/352 [00:22<00:00, 15.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9000000, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model: 100%|██████████| 40/40 [00:02<00:00, 19.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000000, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model: 100%|██████████| 391/391 [00:25<00:00, 15.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000000, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "true, pred, train_loss = test_loop(transformer_model, criterion, train_iterator)\n",
    "act_train = activation['fc'].clone()\n",
    "print(act_train.shape)\n",
    "\n",
    "activation = {'fc': torch.empty((0, 4), device=device)}\n",
    "true, pred, val_loss = test_loop(transformer_model, criterion, val_iterator)\n",
    "act_val = activation['fc'].clone()\n",
    "print(act_val.shape)\n",
    "\n",
    "activation = {'fc': torch.empty((0, 4), device=device)}\n",
    "true, pred, test_loss = test_loop(transformer_model, criterion, test_iterator)\n",
    "act_test = activation['fc'].clone()\n",
    "print(act_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAE(nn.Module):\n",
    "    def __init__(self, latent_size=64):\n",
    "        super(SAE, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.W = nn.Parameter(torch.rand(4, latent_size)) # tied weights for training!\n",
    "        self.b = nn.Parameter(torch.rand(latent_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = F.relu(torch.mm(x, self.W) + self.b)\n",
    "        decoded = torch.mm(encoded, torch.transpose(self.W, 0, 1))\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KSAE(nn.Module):\n",
    "    def __init__(self, latent_size=64, top_k=16):\n",
    "        super(KSAE, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.top_k = top_k\n",
    "        self.W = nn.Parameter(torch.rand(4, latent_size)) # tied weights for training!\n",
    "        self.b = nn.Parameter(torch.rand(latent_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded_acts = F.relu(torch.mm(x, self.W) + self.b)\n",
    "        encoded_topk = torch.topk(encoded_acts, self.top_k, dim=-1)\n",
    "        encoded_topk = torch.zeros_like(encoded_acts).scatter(\n",
    "            -1, encoded_topk.indices, encoded_topk.values\n",
    "        )\n",
    "        decoded = torch.mm(encoded_topk, torch.transpose(self.W, 0, 1))\n",
    "        return decoded\n",
    "\n",
    "    def get_encoded(self, x):\n",
    "        encoded_acts = F.relu(torch.mm(x, self.W) + self.b)\n",
    "        encoded_topk = torch.topk(encoded_acts, self.top_k, dim=-1)\n",
    "        encoded_topk = torch.zeros_like(encoded_acts).scatter(\n",
    "            -1, encoded_topk.indices, encoded_topk.values\n",
    "        )\n",
    "        return encoded_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_sae(model, criterion, optimizer, iterator, epoch, save_every=10):\n",
    "    \"\"\"\n",
    "    This function is used to train a model for one epoch.\n",
    "    :param model: The model to be trained\n",
    "    :param criterion: The loss function\n",
    "    :param optim: The optimizer\n",
    "    :param iterator: The training data iterator\n",
    "    :return: The average loss for this epoch\n",
    "    \"\"\"\n",
    "    model.train() # Is used to put the model in training mode\n",
    "    total_loss = 0\n",
    "    for x in tqdm(iterator, total=len(iterator), desc=\"Training Model\"):\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        # remove this when you add your implementation\n",
    "        \n",
    "        x = x[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        encoded, decoded = model(x)\n",
    "\n",
    "        # print(encoded.shape())\n",
    "        # print(decoded.shape())\n",
    "\n",
    "        loss = criterion(decoded, x, encoded)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop_sae(model, criterion, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate a model on the validation set.\n",
    "    :param model: The model to be evaluated\n",
    "    :param iterator: The validation data iterator\n",
    "    :return: true: a Python boolean array of all the ground truth values\n",
    "             pred: a Python boolean array of all model predictions.\n",
    "            average_loss: The average loss over the validation set\n",
    "    \"\"\"\n",
    "\n",
    "    true, pred = [], []\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    for x in tqdm(iterator, total=len(iterator), desc=\"Evaluating Model\"):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "         # remove this when you add your implementation\n",
    "         x = x[0].to(device)\n",
    "         encoded, decoded = model(x)\n",
    "\n",
    "         loss = criterion(decoded, x, encoded)\n",
    "\n",
    "         total_loss += loss.item()\n",
    "         true.extend(x.tolist())\n",
    "         predicted = decoded\n",
    "         pred.extend(predicted.tolist())\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return true, pred, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop_sae(model, criterion, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate a model on the test set.\n",
    "    :param model: The model to be evaluated\n",
    "    :param iterator: The validation data iterator\n",
    "    :return: true: a Python boolean array of all the ground truth values\n",
    "             pred: a Python boolean array of all model predictions.\n",
    "            average_loss: The average loss over the validation set\n",
    "    \"\"\"\n",
    "\n",
    "    true, pred = [], []\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    for x in tqdm(iterator, total=len(iterator), desc=\"Evaluating Model\"):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "         # remove this when you add your implementation\n",
    "         x = x[0].to(device) \n",
    "         encoded, decoded = model(x)\n",
    "\n",
    "         loss = criterion(decoded, x, encoded)\n",
    "\n",
    "         total_loss += loss.item()\n",
    "         true.extend(x.tolist())\n",
    "         predicted = decoded\n",
    "         pred.extend(predicted.tolist())\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return true, pred, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_ksae(model, criterion, optimizer, iterator, epoch, save_every=10):\n",
    "    \"\"\"\n",
    "    This function is used to train a model for one epoch.\n",
    "    :param model: The model to be trained\n",
    "    :param criterion: The loss function\n",
    "    :param optim: The optimizer\n",
    "    :param iterator: The training data iterator\n",
    "    :return: The average loss for this epoch\n",
    "    \"\"\"\n",
    "    model.train() # Is used to put the model in training mode\n",
    "    total_loss = 0\n",
    "    for x in tqdm(iterator, total=len(iterator), desc=\"Training Model\"):\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        # remove this when you add your implementation\n",
    "        x = x[0].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_hat = model(x)\n",
    "\n",
    "        loss = criterion(x_hat, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop_ksae(model, criterion, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate a model on the validation set.\n",
    "    :param model: The model to be evaluated\n",
    "    :param iterator: The validation data iterator\n",
    "    :return: true: a Python boolean array of all the ground truth values\n",
    "             pred: a Python boolean array of all model predictions.\n",
    "            average_loss: The average loss over the validation set\n",
    "    \"\"\"\n",
    "\n",
    "    true, pred = [], []\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    for x in tqdm(iterator, total=len(iterator), desc=\"Evaluating Model\"):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "         # remove this when you add your implementation\n",
    "         x = x[0].to(device)\n",
    "         x_hat = model(x)\n",
    "\n",
    "         loss = criterion(x_hat, x)\n",
    "\n",
    "         total_loss += loss.item()\n",
    "         true.extend(x.tolist())\n",
    "         predicted = x_hat\n",
    "         pred.extend(predicted.tolist())\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return true, pred, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop_ksae(model, criterion, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate a model on the test set.\n",
    "    :param model: The model to be evaluated\n",
    "    :param iterator: The validation data iterator\n",
    "    :return: true: a Python boolean array of all the ground truth values\n",
    "             pred: a Python boolean array of all model predictions.\n",
    "            average_loss: The average loss over the validation set\n",
    "    \"\"\"\n",
    "\n",
    "    true, pred = [], []\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    for x in tqdm(iterator, total=len(iterator), desc=\"Evaluating Model\"):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "         # remove this when you add your implementation\n",
    "         x = x[0].to(device) \n",
    "         x_hat = model(x)\n",
    "\n",
    "         loss = criterion(x_hat, x)\n",
    "\n",
    "         total_loss += loss.item()\n",
    "         true.extend(x.tolist())\n",
    "         predicted = x_hat\n",
    "         pred.extend(predicted.tolist())\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return true, pred, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ksae_get_encoded(model, iterator):\n",
    "#     \"\"\"\n",
    "#     This function is used to evaluate a model on the test set.\n",
    "#     :param model: The model to be evaluated\n",
    "#     :param iterator: The validation data iterator\n",
    "#     :return: true: a Python boolean array of all the ground truth values\n",
    "#              pred: a Python boolean array of all model predictions.\n",
    "#             average_loss: The average loss over the validation set\n",
    "#     \"\"\"\n",
    "\n",
    "#     pred = []\n",
    "#     pred_stacked = []\n",
    "#     model.eval()\n",
    "#     for x in tqdm(iterator, total=len(iterator), desc=\"Evaluating Model\"):\n",
    "#          x = x[0].to(device) \n",
    "#          x_encoded = model.get_encoded(x)\n",
    "#          pred.extend(x_encoded)\n",
    "#     pred = torch.stack(pred, dim=0)\n",
    "#     return pred\n",
    "\n",
    "def ksae_get_encoded(model, iterator):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "    :param model: The model to be evaluated\n",
    "    :param iterator: The validation data iterator\n",
    "    :return: \n",
    "        pred: A stacked tensor of model predictions\n",
    "        average_loss: The average loss over the validation set\n",
    "    \"\"\"\n",
    "    pred = []\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()  # Use appropriate loss function\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # Prevent gradient calculations\n",
    "        for x, y in tqdm(iterator, total=len(iterator), desc=\"Evaluating Model\"):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x_encoded = model.get_encoded(x)  # Forward pass\n",
    "            loss = criterion(x_encoded, y)\n",
    "            total_loss += loss.item()\n",
    "            pred.append(x_encoded)\n",
    "\n",
    "    pred = torch.stack(pred, dim=0)  # Stack predictions\n",
    "    average_loss = total_loss / len(iterator)  # Compute average loss\n",
    "\n",
    "    return pred, average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae_train_dataset = torch.utils.data.TensorDataset(act_train)\n",
    "sae_val_dataset = torch.utils.data.TensorDataset(act_val)\n",
    "sae_test_dataset = torch.utils.data.TensorDataset(act_test)\n",
    "\n",
    "sae_train_sampler = RandomSampler(sae_train_dataset)\n",
    "sae_val_sampler = RandomSampler(sae_val_dataset)\n",
    "sae_test_sampler  = RandomSampler(sae_test_dataset)\n",
    "\n",
    "sae_train_iterator = DataLoader(sae_train_dataset, batch_size=BATCH_SIZE, sampler=sae_train_sampler)\n",
    "sae_val_iterator = DataLoader(sae_val_dataset, batch_size=BATCH_SIZE, sampler=sae_val_sampler)\n",
    "sae_test_iterator = DataLoader(sae_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# sae_test_iterator  = DataLoader(sae_test_dataset, batch_size=BATCH_SIZE, sampler=sae_test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparams_sae():\n",
    "    LATENT_SIZE = 64\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 20\n",
    "    return LATENT_SIZE, LEARNING_RATE, EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sae_model(latent_size):\n",
    "    model = SAE(\n",
    "        latent_size=latent_size\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguel/miniconda3/envs/nlp_proj/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Training Model: 100%|██████████| 140625/140625 [00:26<00:00, 5366.79it/s]\n",
      "Evaluating Model: 100%|██████████| 15625/15625 [00:02<00:00, 5255.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 -- Train_Loss: 3.264192005845812 -- Val_Loss: 2.562321799636841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  98%|█████████▊| 137429/140625 [00:26<00:00, 5236.20it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m val_loss_over_time_sae \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 13\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop_sae\u001b[49m\u001b[43m(\u001b[49m\u001b[43msae_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msae_train_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     true, pred, val_loss \u001b[38;5;241m=\u001b[39m val_loop_sae(sae_model, criterion, sae_val_iterator) \u001b[38;5;66;03m# change to val\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# accuracy, f1 = get_accuracy_and_f1_score(true, pred)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 18\u001b[0m, in \u001b[0;36mtrain_loop_sae\u001b[0;34m(model, criterion, optimizer, iterator, epoch, save_every)\u001b[0m\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m encoded, decoded \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print(encoded.shape())\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(decoded.shape())\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(decoded, x, encoded)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 10\u001b[0m, in \u001b[0;36mSAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m      9\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(torch\u001b[38;5;241m.\u001b[39mmm(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb)\n\u001b[0;32m---> 10\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(encoded, torch\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded, decoded\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_proj/lib/python3.12/site-packages/torch/nn/modules/module.py:1696\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1698\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LATENT_SIZE, LEARNING_RATE, EPOCHS = get_hyperparams_sae()\n",
    "\n",
    "sae_model = get_sae_model(\n",
    "    latent_size=LATENT_SIZE\n",
    ").to(device)\n",
    "\n",
    "criterion = get_criterion(loss_type='sr')\n",
    "optimizer = get_optimizer(sae_model, LEARNING_RATE)\n",
    "train_loss_over_time_sae = []\n",
    "val_loss_over_time_sae = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_loop_sae(sae_model, criterion, optimizer, sae_train_iterator, epoch, save_every=2)\n",
    "    true, pred, val_loss = val_loop_sae(sae_model, criterion, sae_val_iterator) # change to val\n",
    "    # accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "    print(f\"Epoch {epoch+1} -- Train_Loss: {train_loss} -- Val_Loss: {val_loss}\")\n",
    "    train_loss_over_time_sae.append(train_loss)\n",
    "    val_loss_over_time_sae.append(val_loss)\n",
    "save_checkpoint(sae_model, 'sae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparams_ksae():\n",
    "    LATENT_SIZE = 64\n",
    "    TOP_K = 8\n",
    "    LEARNING_RATE = 0.005\n",
    "    EPOCHS = 4\n",
    "    return LATENT_SIZE, TOP_K, LEARNING_RATE, EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ksae_model(latent_size, top_k):\n",
    "    model = KSAE(\n",
    "        latent_size=latent_size, \n",
    "        top_k=top_k\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_SIZE, TOP_K, LEARNING_RATE, EPOCHS = get_hyperparams_ksae()\n",
    "\n",
    "ksae_model = get_ksae_model(\n",
    "    latent_size=LATENT_SIZE, \n",
    "    top_k=TOP_K\n",
    ").to(device)\n",
    "\n",
    "criterion = get_criterion(loss_type='mse')\n",
    "optimizer = get_optimizer(ksae_model, LEARNING_RATE)\n",
    "train_loss_over_time_ksae = []\n",
    "val_loss_over_time_ksae = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_loop_ksae(ksae_model, criterion, optimizer, sae_train_iterator, epoch, save_every=2)\n",
    "    true, pred, val_loss = val_loop_ksae(ksae_model, criterion, sae_val_iterator) # change to val\n",
    "    # accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "    print(f\"Epoch {epoch+1} -- Train_Loss: {train_loss} -- Val_Loss: {val_loss}\")\n",
    "    train_loss_over_time_ksae.append(train_loss)\n",
    "    val_loss_over_time_ksae.append(val_loss)\n",
    "save_checkpoint(ksae_model, 'ksae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_loss_over_time_ksae, val_loss_over_time_ksae, 'K-Sparse Autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_SIZE, TOP_K, LEARNING_RATE, EPOCHS = get_hyperparams_ksae()\n",
    "\n",
    "ksae_model = get_ksae_model(\n",
    "    latent_size=LATENT_SIZE, \n",
    "    top_k=TOP_K\n",
    ").to(device)\n",
    "\n",
    "load_checkpoint(ksae_model, 'ksae', map_location=device)\n",
    "criterion = get_criterion(loss_type='mse')\n",
    "\n",
    "# evaluate model\n",
    "# true, pred, val_loss = test_loop_ksae(ksae_model, criterion, test_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = next(iter(sae_test_iterator))\n",
    "print(test_data[0][0])\n",
    "\n",
    "test_pred = ksae_model(test_data[0])\n",
    "print(torch.norm(test_data[0] - test_pred))\n",
    "\n",
    "encoded_acts = F.relu(torch.mm(test_data[0], ksae_model.W) + ksae_model.b)\n",
    "encoded_topk = torch.topk(encoded_acts, ksae_model.top_k, dim=-1)\n",
    "encoded_topk = torch.zeros_like(encoded_acts).scatter(-1, encoded_topk.indices, encoded_topk.values)\n",
    "\n",
    "print(encoded_topk[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_activations = ksae_get_encoded(ksae_model, sae_test_iterator)\n",
    "print(latent_activations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_a, act_b = torch.tensor_split(latent_activations, 2, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_a = act_a.to(torch.device(\"cpu\"))\n",
    "act_b = act_b.to(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_a = act_a.clone()\n",
    "torch.save(act_a, \"model_weights/ksae_latent_activations_a.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_b = act_b.clone()\n",
    "torch.save(act_b, \"model_weights/ksae_latent_activations_b.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(act_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(act_a, \"model_weights/ksae_latent_activations_a.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(act_b, \"model_weights/ksae_latent_activations_b.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
