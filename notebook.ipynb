{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Interpreter\n",
    "We train a simple transformer for sentiment analysis on movie reviews, extract interpretable features using SAE and generate explanations using LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataPreprocessing import *\n",
    "import pandas as pd\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "from dataPreprocessing import PADDING_VALUE, UNK_VALUE\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else('mps' if torch.backends.mps.is_available() else 'cpu'))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, model_name, loss_fn='ce'):\n",
    "    file_path = os.path.join(os.getcwd(), 'model_weights', f'checkpoint_{model_name}_{loss_fn}.pt')\n",
    "    os.makedirs(os.path.join(os.getcwd(), 'model_weights'), exist_ok=True)\n",
    "    checkpoint = { # create a dictionary with all the state information\n",
    "        'model_state_dict': model.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, file_path)\n",
    "    print(f\"Checkpoint saved to {file_path}\")\n",
    "\n",
    "def load_checkpoint(model, model_name, loss_fn='ce', map_location='cpu'):\n",
    "    file_path = os.path.join(os.getcwd(), 'model_weights', f'checkpoint_{model_name}_{loss_fn}.pt')\n",
    "    checkpoint = torch.load(file_path, map_location=map_location) # load the checkpoint, ensure correct device\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss_over_time, val_loss_over_time, model_name):\n",
    "    epochs = range(1, len(train_loss_over_time) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_loss_over_time, color='red', label='Train Loss')\n",
    "    plt.plot(epochs, val_loss_over_time, color='blue', label='Val Loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training and Validation Loss for {model_name}')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data\n",
    "Data is loaded from 'dataset' folder. There are 50,000 reviews in the data total. 25,000 for training and 25,000 testing. Reviews have label, either positive or negative. There are an equal number of positive and negative reviews in the each dataset. Each review is a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder path were dataset is located\n",
    "path = 'dataset/'\n",
    "#initialize empty lists to hold data\n",
    "train_pos, train_neg, test_pos, test_neg = [], [], [], []\n",
    "#create a dictionary where the key is the relative path to data and value is empty list\n",
    "sets_dict = {'train/pos/': train_pos, 'train/neg/': train_neg, 'test/pos/': test_pos, 'test/neg/': test_neg}\n",
    "#loop through dictionary to read from files and populate empty lists\n",
    "for dataset in sets_dict:\n",
    "        file_list = [file for file in next(os.walk(os.path.join(path, dataset)))[2] if file.endswith('.txt')]\n",
    "        file_list = sorted(file_list)\n",
    "        load_data(os.path.join(path, dataset), file_list, sets_dict[dataset])\n",
    "#Covert lists to pandas dataframes and combine to form train and test datasets\n",
    "train_data = pd.concat([pd.DataFrame({'review': train_pos, 'label':1}), pd.DataFrame({'review': train_neg, 'label':0})], axis = 0, ignore_index=True)\n",
    "test_data = pd.concat([pd.DataFrame({'review': test_pos, 'label':1}), pd.DataFrame({'review': test_neg, 'label':0})], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize train_data dataframe\n",
    "print(train_data.shape)\n",
    "print(train_data.head())\n",
    "print(train_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize test_data dataframe\n",
    "print(test_data.shape)\n",
    "print(test_data.head())\n",
    "print(test_data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize Data\n",
    "Tokenize each review using spacy english tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"tokenized\"] = train_data[\"review\"].apply(lambda x: tokenize(clean_text(x.lower())))\n",
    "test_data[\"tokenized\"] = test_data[\"review\"].apply(lambda x: tokenize(clean_text(x.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine tokenized reviews\n",
    "print(train_data.head()[\"tokenized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 0\n",
    "total = 0\n",
    "above_thresh = 0\n",
    "for review in train_data[\"tokenized\"]:\n",
    "  if len(review) > max:\n",
    "    max = len(review)\n",
    "  total += len(review)\n",
    "  if len(review) > 800:\n",
    "    above_thresh += 1\n",
    "print(max)\n",
    "print(total/25000)\n",
    "print(above_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Voacb Map\n",
    "Create a vocab map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab, reversed_train_vocab = generate_vocab_map(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator1 = torch.Generator().manual_seed(RANDOM_SEED)\n",
    "\n",
    "train_dataset = ReviewDataset(train_vocab, train_data)\n",
    "train_dataset, val_dataset = random_split(train_dataset,[0.9,0.1], generator=generator1)\n",
    "test_dataset  = ReviewDataset(train_vocab, test_data)\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler = RandomSampler(val_dataset)\n",
    "test_sampler  = RandomSampler(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "val_iterator = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler, collate_fn=collate_fn)\n",
    "test_iterator  = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class SimpleSentimentTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dropout=0.1):\n",
    "        super(SimpleSentimentTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = nn.Embedding(400, embed_dim)\n",
    "        encoder_layers = TransformerEncoderLayer(embed_dim, num_heads, dim_feedforward=embed_dim, dropout=dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def generate_padding_mask(self, src, padding_idx=0):\n",
    "        padding_mask = (src == padding_idx)\n",
    "        return padding_mask\n",
    "\n",
    "    def forward(self, x, padding_idx=0):\n",
    "        padding_mask = self.generate_padding_mask(x, padding_idx)\n",
    "        seq_length = x.size(1)\n",
    "        pos = torch.arange(0, seq_length).unsqueeze(0).repeat(x.size(0), 1).to(x.device)\n",
    "        x = self.embedding(x) + self.pos_encoder(pos)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=padding_mask)\n",
    "        avg_mask = ~padding_mask\n",
    "        avg_mask = avg_mask.permute(1, 0).float().unsqueeze(-1)\n",
    "        x = x * avg_mask\n",
    "        x = x.sum(dim=0) / avg_mask.sum(dim=0).clamp(min=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def train_batch(model, data, targets, padding_idx=0):\n",
    "    outputs = model(data, padding_idx)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(outputs, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_and_f1_score(y_true, y_predicted):\n",
    "    \"\"\"\n",
    "    This function takes in two numpy arrays and computes the accuracy and F1 score\n",
    "    between them. You can use the imported sklearn functions to do this.\n",
    "\n",
    "    Args:\n",
    "        y_true (list) : A 1D numpy array of ground truth labels\n",
    "        y_predicted (list) : A 1D numpy array of predicted labels\n",
    "\n",
    "    Returns:\n",
    "        accuracy (float) : The accuracy of the predictions\n",
    "        f1_score (float) : The F1 score of the predictions\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_predicted)\n",
    "\n",
    "    f1 = f1_score(y_true, y_predicted, average='macro')\n",
    "\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(classes)))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_criterion(loss_type='ce'):\n",
    "    criterion = None\n",
    "\n",
    "    ## YOUR CODE STARTS HERE ##\n",
    "    if loss_type == 'ce':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, learning_rate):\n",
    "    \"\"\"\n",
    "    This function takes a model and a learning rate, and returns an optimizer.\n",
    "    Feel free to experiment with different optimizers.\n",
    "    \"\"\"\n",
    "    optimizer = None\n",
    "\n",
    "    ## YOUR CODE STARTS HERE ##\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    ## YOUR CODE ENDS HERE ##\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, criterion, optimizer, iterator, epoch, save_every=10):\n",
    "    \"\"\"\n",
    "    This function is used to train a model for one epoch.\n",
    "    :param model: The model to be trained\n",
    "    :param criterion: The loss function\n",
    "    :param optim: The optimizer\n",
    "    :param iterator: The training data iterator\n",
    "    :return: The average loss for this epoch\n",
    "    \"\"\"\n",
    "    model.train() # Is used to put the model in training mode\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(iterator, total=len(iterator), desc=\"Training Model\"):\n",
    "        ### YOUR CODE STARTS HERE ###\n",
    "        # remove this when you add your implementation\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "\n",
    "        # output = output.long()\n",
    "        y = y.long()\n",
    "\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        ### YOUR CODE ENDS HERE ###\n",
    "\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(model, criterion, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate a model on the validation set.\n",
    "    :param model: The model to be evaluated\n",
    "    :param iterator: The validation data iterator\n",
    "    :return: true: a Python boolean array of all the ground truth values\n",
    "             pred: a Python boolean array of all model predictions.\n",
    "            average_loss: The average loss over the validation set\n",
    "    \"\"\"\n",
    "\n",
    "    true, pred = [], []\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    for x, y in tqdm(iterator, total=len(iterator), desc=\"Evaluating Model\"):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "         # remove this when you add your implementation\n",
    "         x, y = x.to(device), y.to(device)\n",
    "         output = model(x)\n",
    "\n",
    "        #  output = output.long()\n",
    "         y = y.long()\n",
    "\n",
    "         loss = criterion(output, y)\n",
    "\n",
    "         total_loss += loss.item()\n",
    "         true.extend(y.tolist())\n",
    "         predicted = torch.argmax(output, dim=1)\n",
    "         pred.extend(predicted.tolist())\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return true, pred, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model, criterion, iterator):\n",
    "    \"\"\"\n",
    "    This function is used to evaluate a model on the test set.\n",
    "    :param model: The model to be evaluated\n",
    "    :param iterator: The validation data iterator\n",
    "    :return: true: a Python boolean array of all the ground truth values\n",
    "             pred: a Python boolean array of all model predictions.\n",
    "            average_loss: The average loss over the validation set\n",
    "    \"\"\"\n",
    "\n",
    "    true, pred = [], []\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    for x, y in tqdm(iterator, total=len(iterator), desc=\"Evaluating Model\"):\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "         # remove this when you add your implementation\n",
    "         x, y = x.to(device), y.to(device)\n",
    "         output = model(x)\n",
    "\n",
    "        #  output = output.long()\n",
    "         y = y.long()\n",
    "\n",
    "         loss = criterion(output, y)\n",
    "\n",
    "         total_loss += loss.item()\n",
    "         true.extend(y.tolist())\n",
    "         predicted = torch.argmax(output, dim=1)\n",
    "         pred.extend(predicted.tolist())\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    return true, pred, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparams_transformer():\n",
    "    VOCAB_SIZE = len(train_vocab)\n",
    "    EMBED_DIM = 4\n",
    "    NUM_HEADS = 1\n",
    "    NUM_LAYERS = 1\n",
    "    DROPOUT = 0.1\n",
    "    LEARNING_RATE = 0.015\n",
    "    EPOCHS = 2\n",
    "    return VOCAB_SIZE, EMBED_DIM, NUM_HEADS, NUM_LAYERS, DROPOUT, LEARNING_RATE, EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_model(vocab_size, embedding_dim, num_heads, num_layers, dropout):\n",
    "    model = SimpleSentimentTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embedding_dim,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE, EMBED_DIM, NUM_HEADS, NUM_LAYERS, DROPOUT, LEARNING_RATE, EPOCHS = get_hyperparams_transformer()\n",
    "\n",
    "transformer_model = get_transformer_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "criterion = get_criterion()\n",
    "optimizer = get_optimizer(transformer_model, LEARNING_RATE)\n",
    "train_loss_over_time_transformer = []\n",
    "val_loss_over_time_transformer = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = train_loop(transformer_model, criterion, optimizer, train_iterator, epoch, save_every=2)\n",
    "    true, pred, val_loss = val_loop(transformer_model, criterion, val_iterator) # change to val\n",
    "    accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "    print(f\"Epoch {epoch+1} -- Train_Loss: {train_loss} -- Val_Loss: {val_loss} -- Val_Accuracy: {accuracy} -- Val_F1: {f1}\")\n",
    "    train_loss_over_time_transformer.append(train_loss)\n",
    "    val_loss_over_time_transformer.append(val_loss)\n",
    "save_checkpoint(transformer_model, 'transformer2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_loss_over_time_transformer, val_loss_over_time_transformer, 'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE, EMBED_DIM, NUM_HEADS, NUM_LAYERS, DROPOUT, LEARNING_RATE, EPOCHS = get_hyperparams_transformer()\n",
    "\n",
    "transformer_model = get_transformer_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "load_checkpoint(transformer_model, 'transformer2', map_location=device)\n",
    "\n",
    "# evaluate model\n",
    "true, pred, val_loss = test_loop(transformer_model, criterion, test_iterator)\n",
    "accuracy, f1 = get_accuracy_and_f1_score(true, pred)\n",
    "print(f\"Final Test Accuracy: {accuracy}\")\n",
    "print(f\"Final Test F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(true, pred, classes=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in transformer_model.parameters())\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_specific_weights(checkpoint):\n",
    "    stats = {}\n",
    "    model_weights = checkpoint['model_state_dict']\n",
    "    \n",
    "    # Group weights by component\n",
    "    groups = {\n",
    "        'embedding': ['embedding.weight'],\n",
    "        'positional': ['pos_encoder.weight'],\n",
    "        'attention': [k for k in model_weights.keys() if 'self_attn' in k],\n",
    "        'feedforward': [k for k in model_weights.keys() if 'linear' in k],\n",
    "        'layer_norm': [k for k in model_weights.keys() if 'norm' in k],\n",
    "        'output': [k for k in model_weights.keys() if 'fc.' in k]\n",
    "    }\n",
    "    \n",
    "    for group_name, weight_keys in groups.items():\n",
    "        print(f\"\\n=== {group_name.upper()} WEIGHTS ===\")\n",
    "        \n",
    "        for key in weight_keys:\n",
    "            weight = model_weights[key]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            abs_mean = torch.mean(torch.abs(weight)).item()\n",
    "            std = torch.std(weight).item()\n",
    "            max_val = torch.max(weight).item()\n",
    "            min_val = torch.min(weight).item()\n",
    "            norm = torch.norm(weight).item()\n",
    "            \n",
    "            print(f\"\\n{key}:\")\n",
    "            print(f\"Shape: {weight.shape}\")\n",
    "            print(f\"Magnitude stats:\")\n",
    "            print(f\"  Mean abs value: {abs_mean:.4f}\")\n",
    "            print(f\"  Std deviation:  {std:.4f}\")\n",
    "            print(f\"  Min value:     {min_val:.4f}\")\n",
    "            print(f\"  Max value:     {max_val:.4f}\")\n",
    "            print(f\"  L2 norm:       {norm:.4f}\")\n",
    "            \n",
    "            # Special checks for different component types\n",
    "            if 'norm' in key and 'weight' in key:\n",
    "                if torch.any(weight < 0):\n",
    "                    print(\"  WARNING: Layer norm weights contain negative values!\")\n",
    "                    \n",
    "            if 'bias' in key:\n",
    "                if abs_mean > 1.0:\n",
    "                    print(\"  WARNING: Unusually large bias values!\")\n",
    "                    \n",
    "            if 'self_attn' in key and 'in_proj_weight' in key:\n",
    "                # Check Q, K, V matrices separately\n",
    "                qkv_size = weight.shape[0] // 3\n",
    "                q = weight[:qkv_size]\n",
    "                k = weight[qkv_size:2*qkv_size]\n",
    "                v = weight[2*qkv_size:]\n",
    "                print(\"\\n  QKV breakdown:\")\n",
    "                print(f\"  Q mean abs: {torch.mean(torch.abs(q)).item():.4f}\")\n",
    "                print(f\"  K mean abs: {torch.mean(torch.abs(k)).item():.4f}\")\n",
    "                print(f\"  V mean abs: {torch.mean(torch.abs(v)).item():.4f}\")\n",
    "\n",
    "# Add summary function\n",
    "def print_overall_summary(checkpoint):\n",
    "    model_weights = checkpoint['model_state_dict']\n",
    "    total_params = sum(p.numel() for p in model_weights.values())\n",
    "    max_abs = max(torch.max(torch.abs(p)).item() for p in model_weights.values())\n",
    "    \n",
    "    print(\"\\n=== OVERALL MODEL SUMMARY ===\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Maximum absolute weight value: {max_abs:.4f}\")\n",
    "    print(\"\\nLayer sizes:\")\n",
    "    for name, param in model_weights.items():\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "analyze_specific_weights(checkpoint)\n",
    "# print_overall_summary(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Transformer Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "checkpoint = torch.load(\"model_weights/checkpoint_transformer_ce.pt\", map_location='cpu')\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE, EMBED_DIM, NUM_HEADS, NUM_LAYERS, DROPOUT, LEARNING_RATE, EPOCHS = get_hyperparams_transformer()\n",
    "\n",
    "transformer_model = get_transformer_model(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBED_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "transformer_model.load_state_dict(state_dict=checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(model, criterion, iterator, save_path='test_predictions.pt'):\n",
    "    \"\"\"\n",
    "    Evaluates a model on the test set and saves ground truth labels and predictions.\n",
    "    \n",
    "    Parameters:\n",
    "        model: The PyTorch model to evaluate\n",
    "        criterion: Loss function (e.g., CrossEntropyLoss)\n",
    "        iterator: DataLoader containing test data\n",
    "        save_path: Where to save the predictions\n",
    "    \n",
    "    Returns:\n",
    "        true: List of ground truth labels\n",
    "        pred: List of model predictions\n",
    "        average_loss: Average loss over the test set\n",
    "    \"\"\"\n",
    "    # Initialize lists for storing results\n",
    "    true, pred = [], []\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Disable gradient calculations since we're only doing inference\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(iterator, total=len(iterator), desc=\"Evaluating Model\"):\n",
    "            # Move data to device and get model predictions\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            \n",
    "            # Calculate loss for this batch\n",
    "            y = y.long()\n",
    "            loss = criterion(output, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predicted class for each sample\n",
    "            predicted = torch.argmax(output, dim=1)\n",
    "            \n",
    "            # Store ground truth and predictions\n",
    "            true.extend(y.cpu().tolist())\n",
    "            pred.extend(predicted.cpu().tolist())\n",
    "    \n",
    "    # Calculate average loss across all batches\n",
    "    average_loss = total_loss / len(iterator)\n",
    "    \n",
    "    # Save results dictionary with just predictions and ground truth\n",
    "    results = {\n",
    "        'ground_truth': true,\n",
    "        'predictions': pred\n",
    "    }\n",
    "    torch.save(results, save_path)\n",
    "    \n",
    "    return true, pred, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Model: 100%|██████████| 391/391 [00:08<00:00, 45.52it/s]\n"
     ]
    }
   ],
   "source": [
    "true_labels, predictions, avg_loss = test_loop(\n",
    "    model=transformer_model,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    iterator=test_iterator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
