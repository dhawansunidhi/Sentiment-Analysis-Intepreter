{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from dataPreprocessing import *\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder path were dataset is located\n",
    "path = 'dataset/'\n",
    "#initialize empty lists to hold data\n",
    "train_pos, train_neg, test_pos, test_neg = [], [], [], []\n",
    "#create a dictionary where the key is the relative path to data and value is empty list\n",
    "sets_dict = {'train/pos/': train_pos, 'train/neg/': train_neg, 'test/pos/': test_pos, 'test/neg/': test_neg}\n",
    "#loop through dictionary to read from files and populate empty lists\n",
    "for dataset in sets_dict:\n",
    "        file_list = [file for file in os.listdir(os.path.join(path, dataset)) if file.endswith('.txt')]\n",
    "        file_list = sorted(file_list)\n",
    "        load_data(os.path.join(path, dataset), file_list, sets_dict[dataset])\n",
    "#Covert lists to pandas dataframes and combine to form train and test datasets\n",
    "train_data = pd.concat([pd.DataFrame({'review': train_pos, 'label':1}), pd.DataFrame({'review': train_neg, 'label':0})], axis = 0, ignore_index=True)\n",
    "test_data = pd.concat([pd.DataFrame({'review': test_pos, 'label':1}), pd.DataFrame({'review': test_neg, 'label':0})], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"tokenized\"] = test_data[\"review\"].apply(lambda x: tokenize(clean_text(x.lower())))\n",
    "#Examine tokenized reviews\n",
    "print(test_data.head()[\"tokenized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_trim(tokens, length=400, pad_token=\"\"):\n",
    "    \"\"\"Pads or trims a list of tokens to a specified length.\"\"\"\n",
    "    if len(tokens) > length:\n",
    "        # Trim the list to the desired length\n",
    "        return tokens[:length]\n",
    "    else:\n",
    "        # Pad the list with the pad_token\n",
    "        return tokens + [pad_token] * (length - len(tokens))\n",
    "\n",
    "# Apply the function to create the tokenized column\n",
    "test_data[\"tokenized\"] = test_data[\"review\"].apply(\n",
    "    lambda x: pad_or_trim(tokenize(clean_text(x.lower())))\n",
    ")\n",
    "\n",
    "# Verify the length of each tokenized review\n",
    "print(test_data[\"tokenized\"].apply(len).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.head()[\"tokenized\"][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the .pt files\n",
    "# file_path_a = os.path.join('model_weights', 'ksae_latent_activations_a.pt')\n",
    "# file_path_b = os.path.join('model_weights', 'ksae_latent_activations_b.pt')\n",
    "# data_a = torch.load(file_path_a)\n",
    "# data_b = torch.load(file_path_b)\n",
    "# # Combine the tensors along the first dimension (dim=0)\n",
    "# data = torch.cat((data_a, data_b), dim=0)\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join('model_weights', 'ksae_latent_activations.pt')\n",
    "data = torch.load(file_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data (25000 reviews, 400 tokens, 64 features)\n",
    "data = data.reshape(25000,400,64)\n",
    "print(data.shape)\n",
    "\n",
    "# Compute the maximum weight for each feature across all tokens (dim=1 aggregates across tokens)\n",
    "max_weights = data.max(dim=1).values  # Shape: (25000, 64)\n",
    "\n",
    "# Print the maximum weight for each feature across all tokens\n",
    "print(max_weights)\n",
    "\n",
    "# If you want to further summarize it (e.g., max weight per feature across all reviews), you can use:\n",
    "max_weights_per_feature = max_weights.max(dim=0).values  # Shape: (64,)\n",
    "print(\"Maximum weight for each feature across all tokens and reviews:\")\n",
    "print(max_weights_per_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[24990][10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[3][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[3][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_tokens(feature_number, data, tokenized_data):\n",
    "    \"\"\"\n",
    "    Prints the tokens with the maximum activation for a given feature across all reviews,\n",
    "    along with their activation values.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_number: int, the feature index to analyze (0-63).\n",
    "    - data: tensor of shape (25000, 400, 64) containing activations.\n",
    "    - tokenized_data: list of tokenized reviews (each review is a list of 400 tokens).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the feature number is within valid range\n",
    "    if feature_number < 0 or feature_number >= data.shape[2]:\n",
    "        raise ValueError(f\"Feature number must be between 0 and {data.shape[2] - 1}\")\n",
    "\n",
    "    # Extract the activations for the given feature across all tokens and reviews\n",
    "    activations = data[:, :, feature_number]  # Shape: (25000, 400)\n",
    "\n",
    "    # Find the index of the maximum activation per review\n",
    "    max_token_indices = activations.argmax(dim=1)  # Shape: (25000,)\n",
    "\n",
    "    # Collect the corresponding tokens with max activation for each review\n",
    "    max_tokens = [\n",
    "        (tokenized_data[i][max_token_indices[i].item()], activations[i, max_token_indices[i]].item())\n",
    "        for i in range(len(tokenized_data))\n",
    "    ]\n",
    "\n",
    "    # Print the tokens with the highest activation for the specified feature\n",
    "    print(f\"Tokens with max activation for feature {feature_number}:\")\n",
    "    for i, (token, activation) in enumerate(max_tokens[:50]):  # Show first 10 results for brevity\n",
    "        print(f\"Review {i + 1}: Token = '{token}', Activation = {activation:.4f}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assume `test_data[\"tokenized\"]` contains the tokenized reviews.\n",
    "tokenized_data = test_data[\"tokenized\"].tolist()\n",
    "get_max_tokens(15, data, tokenized_data)  # Replace 0 with the feature number you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_colored_tokens_with_more_spacing(feature_number, review_number, data, tokenized_data):\n",
    "    \"\"\"\n",
    "    Plots tokens with color-coded backgrounds based on their activation values,\n",
    "    with more spacing between tokens and using a blue-to-red colormap (cool to hot).\n",
    "\n",
    "    Parameters:\n",
    "    - feature_number: int, the feature index to analyze (0-63).\n",
    "    - review_number: int, the review index to analyze (0-24999).\n",
    "    - data: tensor of shape (25000, 400, 64) containing activations.\n",
    "    - tokenized_data: list of tokenized reviews (each review is a list of 400 tokens).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure feature and review numbers are within valid ranges\n",
    "    if feature_number < 0 or feature_number >= data.shape[2]:\n",
    "        raise ValueError(f\"Feature number must be between 0 and {data.shape[2] - 1}\")\n",
    "    if review_number < 0 or review_number >= data.shape[0]:\n",
    "        raise ValueError(f\"Review number must be between 0 and {data.shape[0] - 1}\")\n",
    "\n",
    "    # Extract activations for the given feature and review\n",
    "    activations = data[review_number, :, feature_number].detach().numpy()  # Shape: (400,)\n",
    "\n",
    "    # Normalize activations for color mapping\n",
    "    norm = plt.Normalize(activations.min(), activations.max())\n",
    "    cmap = plt.cm.coolwarm  # Blue to red colormap\n",
    "\n",
    "    # Create the plot with more spacing\n",
    "    fig, ax = plt.subplots(figsize=(25, 5))  # Increased width for better spacing\n",
    "\n",
    "    # Adjust spacing between tokens\n",
    "    x_positions = np.arange(len(tokenized_data[review_number])) * 10  # Add more space between tokens\n",
    "\n",
    "    # Display tokens with background colors based on their activation\n",
    "    for i, token in enumerate(tokenized_data[review_number]):\n",
    "        color = cmap(norm(activations[i]))\n",
    "        ax.text(x_positions[i], 0.5, token, ha='center', va='center', fontsize=12,\n",
    "                bbox=dict(facecolor=color, edgecolor='none', boxstyle='round,pad=0.5'))\n",
    "\n",
    "    # Set plot limits and remove axis for a cleaner look\n",
    "    plt.xlim(-1, x_positions[-1] + 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.title(f'Token Activation Visualization for Feature {feature_number}, Review {review_number}', fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_colored_tokens_with_more_spacing(8, 0, data, tokenized_data)  # Replace with your feature and review numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def generate_html_colored_tokens(feature_number, review_number, data, tokenized_data):\n",
    "    \"\"\"\n",
    "    Generates HTML to display tokens with color-coded backgrounds based on their activation values.\n",
    "    Uses a blue-to-red gradient for coloring.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_number: int, the feature index to analyze (0-63).\n",
    "    - review_number: int, the review index to analyze (0-24999).\n",
    "    - data: tensor of shape (25000, 400, 64) containing activations.\n",
    "    - tokenized_data: list of tokenized reviews (each review is a list of 400 tokens).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure feature and review numbers are within valid ranges\n",
    "    if feature_number < 0 or feature_number >= data.shape[2]:\n",
    "        raise ValueError(f\"Feature number must be between 0 and {data.shape[2] - 1}\")\n",
    "    if review_number < 0 or review_number >= data.shape[0]:\n",
    "        raise ValueError(f\"Review number must be between 0 and {data.shape[0] - 1}\")\n",
    "\n",
    "    # Extract activations for the given feature and review\n",
    "    activations = data[review_number, :, feature_number].detach().numpy()  # Shape: (400,)\n",
    "\n",
    "    # Normalize activations for color mapping\n",
    "    norm = mcolors.Normalize(activations.min(), activations.max())\n",
    "    cmap = cm.coolwarm  # Blue-to-red colormap\n",
    "\n",
    "    # Build the HTML content\n",
    "    html_content = '<div style=\"display: flex; flex-wrap: wrap; line-height: 2;\">'\n",
    "    for i, token in enumerate(tokenized_data[review_number]):\n",
    "        if token:\n",
    "            color = mcolors.rgb2hex(cmap(norm(activations[i]))[:3])  # Convert to hex color\n",
    "            html_content += f'<span style=\"background-color: {color}; padding: 5px 10px; margin: 2px; border-radius: 5px; color:black\">{token}</span>'\n",
    "    html_content += '</div>'\n",
    "\n",
    "    # Display the HTML content\n",
    "    display(HTML(html_content))\n",
    "\n",
    "# Example usage:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "\n",
    "def generate_html_top_2_tokens(feature_number, data, tokenized_data):\n",
    "    \"\"\"\n",
    "    Generates HTML to display the top 2 tokens with the highest activations for each review,\n",
    "    ignoring empty or whitespace-only tokens.\n",
    "    Uses a blue-to-red gradient for coloring.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_number: int, the feature index to analyze (0-63).\n",
    "    - data: tensor of shape (25000, 400, 64) containing activations.\n",
    "    - tokenized_data: list of tokenized reviews (each review is a list of 400 tokens).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the feature number is within the valid range\n",
    "    if feature_number < 0 or feature_number >= data.shape[2]:\n",
    "        raise ValueError(f\"Feature number must be between 0 and {data.shape[2] - 1}\")\n",
    "\n",
    "    # Normalize activations for color mapping\n",
    "    cmap = cm.coolwarm  # Blue-to-red colormap\n",
    "    all_activations = data[:, :, feature_number].detach().numpy()  # Extract activations for the feature\n",
    "    norm = mcolors.Normalize(all_activations.min(), all_activations.max())  # Normalize activations\n",
    "\n",
    "    # Build the HTML content\n",
    "    html_content = '<div style=\"display: flex; flex-wrap: wrap; line-height: 2;\">'\n",
    "\n",
    "    for review_idx, review in enumerate(tokenized_data):\n",
    "        # Get activations for the current review\n",
    "        activations = all_activations[review_idx]  # Shape: (400,)\n",
    "\n",
    "        # Filter out empty or whitespace-only tokens and collect their indices\n",
    "        valid_tokens = [(i, token) for i, token in enumerate(review) if token.strip()]\n",
    "        \n",
    "        # Get the activations corresponding to valid tokens\n",
    "        valid_activations = [activations[i] for i, _ in valid_tokens]\n",
    "\n",
    "        # If fewer than 2 valid tokens, skip this review\n",
    "        if len(valid_activations) < 2:\n",
    "            continue\n",
    "\n",
    "        # Find the top 2 valid token indices based on activations\n",
    "        top_2_indices = sorted(range(len(valid_activations)), key=lambda i: valid_activations[i], reverse=True)[:2]\n",
    "\n",
    "        # Add the top 2 tokens to the HTML content\n",
    "        for idx in top_2_indices:\n",
    "            token_idx, token = valid_tokens[idx]\n",
    "            activation = valid_activations[idx]\n",
    "            color = mcolors.rgb2hex(cmap(norm(activation))[:3])  # Convert to hex color\n",
    "            html_content += f'<span style=\"background-color: {color}; padding: 5px 10px; margin: 5px; border-radius: 5px; color:black\">{token}</span>'\n",
    "\n",
    "    html_content += '</div>'\n",
    "\n",
    "    # Display the HTML content\n",
    "    display(HTML(html_content))\n",
    "\n",
    "# Example usage:\n",
    "generate_html_top_2_tokens(8, data[:, 0:len(data)//2], tokenized_data)  # Replace 8 with the desired feature number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "\n",
    "def generate_html_heatmap_top_reviews(feature_number, top_n_reviews, data, tokenized_data):\n",
    "    \"\"\"\n",
    "    Generates an HTML heatmap for all tokens in the top reviews with the highest activations for a specific feature.\n",
    "    Uses a blue-to-red gradient for coloring.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_number: int, the feature index to analyze (0-63).\n",
    "    - top_n_reviews: int, the number of reviews to display based on highest activation scores.\n",
    "    - data: tensor of shape (25000, 400, 64) containing activations.\n",
    "    - tokenized_data: list of tokenized reviews (each review is a list of 400 tokens).\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the feature number is within the valid range\n",
    "    if feature_number < 0 or feature_number >= data.shape[2]:\n",
    "        raise ValueError(f\"Feature number must be between 0 and {data.shape[2] - 1}\")\n",
    "\n",
    "    # Extract activations for the feature\n",
    "    activations = data[:, :, feature_number].detach().numpy()  # Shape: (25000, 400)\n",
    "\n",
    "    # Calculate the average activation per review for the specified feature\n",
    "    avg_activations = activations.mean(axis=1)  # Shape: (25000,)\n",
    "\n",
    "    # Find the indices of the top reviews with the highest average activation\n",
    "    top_review_indices = avg_activations.argsort()[-top_n_reviews:][::-1]\n",
    "\n",
    "    # Normalize activations for color mapping\n",
    "    cmap = cm.coolwarm  # Blue-to-red colormap\n",
    "    norm = mcolors.Normalize(activations.min(), activations.max())  # Normalize across all activations\n",
    "\n",
    "    # Build the HTML content\n",
    "    html_content = '<div style=\"display: flex; flex-direction: column; gap: 15px; max-width: 100%; line-height: 4; flex-wrap: wrap;\">'\n",
    "\n",
    "    for review_idx in top_review_indices:\n",
    "        # Get the tokens and activations for the current top review\n",
    "        review_tokens = tokenized_data[review_idx]\n",
    "        review_activations = activations[review_idx]  # Shape: (400,)\n",
    "\n",
    "        # Filter out empty or whitespace-only tokens and collect their indices\n",
    "        valid_tokens = [(i, token) for i, token in enumerate(review_tokens) if token.strip()]\n",
    "        \n",
    "        # Get the activations corresponding to valid tokens\n",
    "        valid_activations = [review_activations[i] for i, _ in valid_tokens]\n",
    "\n",
    "        # Add tokens to the HTML content with color based on activation\n",
    "        html_content += '<div style=\"margin-bottom: 10px; line-height: 2.5;\">'\n",
    "        for i, token in enumerate(valid_tokens):\n",
    "            token_idx, token = token\n",
    "            activation = valid_activations[i]\n",
    "            color = mcolors.rgb2hex(cmap(norm(activation))[:3])  # Convert to hex color\n",
    "            html_content += f'<span style=\"background-color: {color}; padding: 5px 10px; margin: 5px; border-radius: 5px; color:black; display:inline-block; max-width: 100%; overflow-wrap: break-word;\">{token}</span>'\n",
    "        html_content += '</div>'\n",
    "\n",
    "    html_content += '</div>'\n",
    "\n",
    "    # Display the HTML content\n",
    "    display(HTML(html_content))\n",
    "\n",
    "# Example usage:\n",
    "generate_html_heatmap_top_reviews(43, 25, data, tokenized_data)  # Replace 8 with the desired feature number, and 10 with the number of top reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Ensure PyTorch is imported\n",
    "\n",
    "def get_max_tokens(feature_number, data, tokenized_data, start=0, end=1000):\n",
    "    \"\"\"\n",
    "    Returns the tokens with the maximum activation for a given feature across all reviews,\n",
    "    along with their activation values, including empty tokens (whitespace-only).\n",
    "\n",
    "    Parameters:\n",
    "    - feature_number: int, the feature index to analyze (0-63).\n",
    "    - data: tensor of shape (25000, 400, 64) containing activations.\n",
    "    - tokenized_data: list of tokenized reviews (each review is a list of 400 tokens).\n",
    "    - start: int, the starting index for displaying results (default=0).\n",
    "    - end: int, the ending index for displaying results (default=1000).\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples: (token, activation_value) for each review's highest activated token.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the feature number is within a valid range\n",
    "    if feature_number < 0 or feature_number >= data.shape[2]:\n",
    "        raise ValueError(f\"Feature number must be between 0 and {data.shape[2] - 1}\")\n",
    "\n",
    "    # Extract the activations for the given feature across all tokens and reviews\n",
    "    activations = data[:, :, feature_number]  # Shape: (25000, 400)\n",
    "\n",
    "    # Find the index of the maximum activation per review\n",
    "    max_token_indices = activations.argmax(dim=1)  # Shape: (25000,)\n",
    "\n",
    "    # Collect the corresponding tokens with max activation for each review\n",
    "    max_tokens = [\n",
    "        (tokenized_data[i][idx.item()], activations[i, idx].item())\n",
    "        for i, idx in enumerate(max_token_indices)\n",
    "    ]\n",
    "\n",
    "    # Display the first 10 results for verification (within the specified range)\n",
    "    for i, (token, activation) in enumerate(max_tokens[start:end]):\n",
    "        print(f\"{start + i + 1}: Token = '{token}', Activation = {activation:.4f}\")\n",
    "\n",
    "    return max_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = get_max_tokens(8, data, tokenized_data,0,12501)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_max_tokens(24, data, tokenized_data, 0,12501)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_max_tokens(32, data, tokenized_data,0,12501)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_max_tokens(36, data, tokenized_data, 0, 12501) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_max_tokens(44, data, tokenized_data,0,12501)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_max_tokens(47, data, tokenized_data,12499,25000)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_max_tokens(54, data, tokenized_data, 0,12501)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_n_tokens_overall(feature_number, data, tokenized_data, n=10):\n",
    "    \"\"\"\n",
    "    Prints the top n tokens with the highest activations across all reviews,\n",
    "    ignoring empty or whitespace-only tokens.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_number: int, the feature index to analyze (0-63).\n",
    "    - data: tensor of shape (25000, 400, 64) containing activations.\n",
    "    - tokenized_data: list of tokenized reviews (each review is a list of 400 tokens).\n",
    "    - n: int, the number of top tokens to print.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the feature number is within the valid range\n",
    "    if feature_number < 0 or feature_number >= data.shape[2]:\n",
    "        raise ValueError(f\"Feature number must be between 0 and {data.shape[2] - 1}\")\n",
    "\n",
    "    # Extract activations for the specified feature\n",
    "    all_activations = data[:, :, feature_number].detach().numpy()  # Shape: (25000, 400)\n",
    "\n",
    "    # Store tokens and activations in a list of tuples (activation, token)\n",
    "    tokens_activations = []\n",
    "\n",
    "    for review_idx, review in enumerate(tokenized_data):\n",
    "        activations = all_activations[review_idx]  # Shape: (400,)\n",
    "\n",
    "        # Filter out empty or whitespace-only tokens\n",
    "        valid_tokens = [(i, token) for i, token in enumerate(review) if token.strip()]\n",
    "        \n",
    "        # Get the activations corresponding to valid tokens and store them\n",
    "        tokens_activations.extend([(activations[i], token) for i, token in valid_tokens])\n",
    "\n",
    "    # Sort the list by activation values in descending order and get the top n tokens\n",
    "    top_n_tokens = sorted(tokens_activations, key=lambda x: x[0], reverse=True)[:n]\n",
    "\n",
    "    # Print the top n tokens and their activation values\n",
    "    print(f\"Top {n} tokens overall for feature {feature_number}:\")\n",
    "    for activation, token in top_n_tokens:\n",
    "        print(f\"  Token: '{token}', Activation: {activation:.4f}\")\n",
    "\n",
    "# 15, 26, 43, 44, 48, 49, 52\n",
    "# Example usage:\n",
    "print_top_n_tokens_overall(48, data, tokenized_data, n=500)  # Replace 8 with the desired feature number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for analyzing model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join('model_weights', 'ksae_latent_activations.pt')\n",
    "data = torch.load(file_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feature_means, reference_list\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m feature_means, reference_list \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_global_feature_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenized_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGlobal Feature Statistics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m reference_list:\n",
      "Cell \u001b[0;32mIn[107], line 28\u001b[0m, in \u001b[0;36mcalculate_global_feature_data\u001b[0;34m(data, tokenized_data, feature_list)\u001b[0m\n\u001b[1;32m     21\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\n\u001b[1;32m     22\u001b[0m     [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m review]\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m tokenized_data\n\u001b[1;32m     24\u001b[0m ], device\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Extract relevant features and apply padding mask\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Shape: (25000, 400, len(feature_list))\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m feature_activations \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     29\u001b[0m feature_activations \u001b[38;5;241m=\u001b[39m feature_activations \u001b[38;5;241m*\u001b[39m padding_mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Create non-zero mask (threshold at 0.001)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "def calculate_global_feature_data(data, tokenized_data, feature_list=[15, 26, 43, 44, 48, 49, 52]):\n",
    "    \"\"\"\n",
    "    Calculates mean non-zero activations for features using vectorized operations.\n",
    "    \n",
    "    This optimized version avoids loops by leveraging PyTorch's tensor operations,\n",
    "    which are implemented in C++ and can utilize GPU acceleration if available.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Tensor of shape (25000, 400, 64) containing activations\n",
    "    - tokenized_data: List of tokenized reviews\n",
    "    - feature_list: List of feature indices to analyze\n",
    "    \n",
    "    Returns:\n",
    "    - feature_means: Dictionary of mean activations per feature\n",
    "    - reference_list: List of formatted reference strings\n",
    "    \"\"\"\n",
    "    # Convert feature list to tensor for efficient indexing\n",
    "    feature_indices = torch.tensor(feature_list, device=data.device)\n",
    "    \n",
    "    # Create padding mask\n",
    "    padding_mask = torch.tensor([\n",
    "        [1 if token != '[PAD]' else 0 for token in review]\n",
    "        for review in tokenized_data\n",
    "    ], device=data.device)\n",
    "    \n",
    "    # Extract relevant features and apply padding mask\n",
    "    # Shape: (25000, 400, len(feature_list))\n",
    "    feature_activations = data[:, :, feature_indices]\n",
    "    feature_activations = feature_activations * padding_mask.unsqueeze(-1)\n",
    "    \n",
    "    # Create non-zero mask (threshold at 0.001)\n",
    "    active_mask = (torch.abs(feature_activations) >= 0.001).float()\n",
    "    \n",
    "    # Calculate sums and counts efficiently\n",
    "    total_activations = (feature_activations * active_mask).sum(dim=(0, 1))\n",
    "    activation_counts = active_mask.sum(dim=(0, 1))\n",
    "    \n",
    "    # Calculate means (avoiding division by zero)\n",
    "    means = torch.where(\n",
    "        activation_counts > 0,\n",
    "        total_activations / activation_counts,\n",
    "        torch.zeros_like(total_activations)\n",
    "    )\n",
    "    \n",
    "    # Convert results to Python structures\n",
    "    feature_means = {\n",
    "        feature_list[i]: means[i].item() \n",
    "        for i in range(len(feature_list))\n",
    "    }\n",
    "    \n",
    "    # Create reference strings with count information\n",
    "    reference_list = [\n",
    "        f\"Feature {feature_list[i]}: mean activation = {means[i]:.3f} \"\n",
    "        f\"(from {int(activation_counts[i].item())} active tokens)\"\n",
    "        for i in range(len(feature_list))\n",
    "    ]\n",
    "    \n",
    "    return feature_means, reference_list\n",
    "\n",
    "# Example usage:\n",
    "feature_means, reference_list = calculate_global_feature_data(data, tokenized_data)\n",
    "\n",
    "print(\"Global Feature Statistics:\")\n",
    "for ref in reference_list:\n",
    "    print(ref)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_review(review_index, data, tokenized_data, feature_means, \n",
    "                                feature_list=[15, 26, 43, 44, 48, 49, 52]):\n",
    "    \"\"\"\n",
    "    Analyzes a review's feature activations and compares them to the global means.\n",
    "    This helps us understand when a feature is behaving typically or unusually in a specific review.\n",
    "    \"\"\"\n",
    "    review_activations = data[review_index]\n",
    "    num_tokens = len(tokenized_data[review_index])\n",
    "    \n",
    "    print(f\"Analyzing review #{review_index}\")\n",
    "    \n",
    "    for feature in feature_list:\n",
    "        non_padding_count = 0\n",
    "        zero_count = 0\n",
    "        active_count = 0\n",
    "        non_zero_activations = []\n",
    "        \n",
    "        for position in range(num_tokens):\n",
    "            if tokenized_data[review_index][position] != '[PAD]':\n",
    "                activation = review_activations[position, feature].item()\n",
    "                non_padding_count += 1\n",
    "                if abs(activation) < 0.001:\n",
    "                    zero_count += 1\n",
    "                else:\n",
    "                    active_count += 1\n",
    "                    non_zero_activations.append(activation)\n",
    "        \n",
    "        if non_zero_activations:\n",
    "            mean_activation = sum(non_zero_activations) / len(non_zero_activations)\n",
    "            diff_from_mean = mean_activation - feature_means[feature]\n",
    "        else:\n",
    "            mean_activation = 0\n",
    "            diff_from_mean = -feature_means[feature]\n",
    "        \n",
    "        print(f\"\\nFeature {feature}:\")\n",
    "        print(f\"Active tokens: {active_count}/{non_padding_count} ({(active_count/non_padding_count)*100:.1f}%)\")\n",
    "        print(f\"Zero tokens: {zero_count}/{non_padding_count} ({(zero_count/non_padding_count)*100:.1f}%)\")\n",
    "        \n",
    "        if non_zero_activations:\n",
    "            print(f\"Mean activation in review: {mean_activation:.3f}\")\n",
    "            print(f\"Global mean activation: {feature_means[feature]:.3f}\")\n",
    "            # print(f\"Difference from global: {diff_from_mean:+.3f}\")\n",
    "        else:\n",
    "            print(\"No active tokens in this review\")\n",
    "            print(f\"Global mean activation: {feature_means[feature]:.3f}\")\n",
    "            \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start your analysis here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the review #s you're analyzing\n",
    "reviews = np.array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing review #19\n",
      "\n",
      "Feature 15:\n",
      "Active tokens: 265/400 (66.2%)\n",
      "Zero tokens: 135/400 (33.8%)\n",
      "Mean activation in review: 1.364\n",
      "Global mean activation: 1.207\n",
      "--------------------------------------------------\n",
      "\n",
      "Feature 26:\n",
      "Active tokens: 135/400 (33.8%)\n",
      "Zero tokens: 265/400 (66.2%)\n",
      "Mean activation in review: 1.391\n",
      "Global mean activation: 1.819\n",
      "--------------------------------------------------\n",
      "\n",
      "Feature 43:\n",
      "Active tokens: 149/400 (37.2%)\n",
      "Zero tokens: 251/400 (62.7%)\n",
      "Mean activation in review: 0.766\n",
      "Global mean activation: 0.764\n",
      "--------------------------------------------------\n",
      "\n",
      "Feature 44:\n",
      "Active tokens: 251/400 (62.7%)\n",
      "Zero tokens: 149/400 (37.2%)\n",
      "Mean activation in review: 0.798\n",
      "Global mean activation: 0.781\n",
      "--------------------------------------------------\n",
      "\n",
      "Feature 48:\n",
      "Active tokens: 400/400 (100.0%)\n",
      "Zero tokens: 0/400 (0.0%)\n",
      "Mean activation in review: 0.139\n",
      "Global mean activation: 0.123\n",
      "--------------------------------------------------\n",
      "\n",
      "Feature 49:\n",
      "Active tokens: 185/400 (46.2%)\n",
      "Zero tokens: 215/400 (53.8%)\n",
      "Mean activation in review: 2.369\n",
      "Global mean activation: 2.142\n",
      "--------------------------------------------------\n",
      "\n",
      "Feature 52:\n",
      "Active tokens: 215/400 (53.8%)\n",
      "Zero tokens: 185/400 (46.2%)\n",
      "Mean activation in review: 2.546\n",
      "Global mean activation: 2.255\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run this cell for every review you're analyzing\n",
    "review_idx = 19\n",
    "\n",
    "analyze_review(review_idx, data, tokenized_data, feature_means)\n",
    "\n",
    "# Feature 15: Positive\n",
    "# Feature 26: Negative/Disgust\n",
    "# Feature 43: Review Structure/Mixed\n",
    "# Feature 44: Narrative elements\n",
    "# Feature 48: Film genre\n",
    "# Feature 49: Film craftsmenship/plot\n",
    "# Feature 52: Eeriness/eerie environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sunidhi's analysis\n",
    "# Review 12: negative, higher non zero activation for negative than positive, higher mean than global for eeriness, moderate for everything else\n",
    "# Review 13: negative, negative is closer to global mean, high eerieness mean\n",
    "# Review 14: positive, 72% active for positive vs 25% for negative, high eeriness but still\n",
    "# Review 15: postitive, 72% active + review mean higher than global mean\n",
    "# Review 16: negative, higher active tokens\n",
    "# Review 17: positive, lower mean for negative, equal mean for positive\n",
    "# Review 18: positive, same\n",
    "# Review 19: same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in your predictions for the 10 reviews\n",
    "your_predictions = np.array([1, 1, 0, 0, 1, 1, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell after you've filled out ALL your predictions and want to check your accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved results\n",
    "results = torch.load('model_weights/test_predictions.pt')\n",
    "\n",
    "# Access the predictions and ground truth labels\n",
    "ground_truth = np.array(results['ground_truth'])\n",
    "model_predictions = np.array(results['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews analyzed:  [10 11 12 13 14 15 16 17 18 19]\n",
      "Your Predictions:  [1 1 0 0 1 1 0 1 1 1]\n",
      "Model Predictions:  [1 1 0 0 1 0 0 1 1 0]\n",
      "Ground Truth:  [1 1 0 0 1 0 0 1 1 1]\n",
      "Your Accuracy against model:  0.8\n",
      "Your Accuracy against labels:  0.9\n",
      "Model's accuracy against labels:  0.9\n"
     ]
    }
   ],
   "source": [
    "model_preds = model_predictions[reviews]\n",
    "labels = ground_truth[reviews]\n",
    "print(\"Reviews analyzed: \", reviews)\n",
    "print(\"Your Predictions: \", your_predictions)\n",
    "print(\"Model Predictions: \", model_preds)\n",
    "print(\"Ground Truth: \", labels)\n",
    "print(\"Your Accuracy against model: \", np.mean(your_predictions==model_preds))\n",
    "print(\"Your Accuracy against labels: \", np.mean(your_predictions==labels))\n",
    "print(\"Model's accuracy against labels: \", np.mean(model_preds==labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
